{"cells":[{"cell_type":"markdown","source":["# PyTorch에서 처음부터 트레이닝 루프 작성하기"],"metadata":{"id":"EP3KRhZrLbXh"}},{"cell_type":"markdown","source":["**저자:** [fchollet](https://twitter.com/fchollet)  \n","**생성일:** 2023/06/25  \n","**최종편집일:** 2023/06/25  \n","**설명:** PyTorch에서 낮은 레벨의 트레이닝 및 평가 루프 작성하기"],"metadata":{"id":"VyFcFZ7CLc9T"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Y5VCzvf5LIc2","executionInfo":{"status":"ok","timestamp":1726966254844,"user_tz":-540,"elapsed":2,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["# !pip install keras --upgrade --quiet"]},{"cell_type":"markdown","metadata":{"id":"RobUQxH3LIc3"},"source":["## 셋업"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CbWgmUEDLIc4","executionInfo":{"status":"ok","timestamp":1726966256779,"user_tz":-540,"elapsed":1934,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["import os\n","\n","# This guide can only be run with the torch backend.\n","os.environ[\"KERAS_BACKEND\"] = \"torch\"\n","\n","import torch\n","import keras\n","import numpy as np"]},{"cell_type":"code","source":["from keras import backend\n","print(backend.backend())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9O4XZQHNLflZ","executionInfo":{"status":"ok","timestamp":1726966256786,"user_tz":-540,"elapsed":6,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"0189f0b7-3013-4309-de35-b20467e43b54"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["torch\n"]}]},{"cell_type":"markdown","source":["## 소개"],"metadata":{"id":"Uzbhs79mLk7C"}},{"cell_type":"markdown","source":["Keras는 `fit()`과 `evaluate()`라는 기본 트레이닝 및 평가 루프를 제공합니다.\n","이들의 사용법은 [빌트인 메서드를 사용한 트레이닝 및 평가](https://codecompose7.github.io/keras-doc-kr.github.io/guides/training_with_built_in_methods/) 가이드에서 다룹니다.\n","\n","모델의 학습 알고리즘을 커스터마이즈하면서도 `fit()`의 편리함을 활용하고 싶다면\n","(예를 들어, `fit()`을 사용해 GAN을 트레이닝하려는 경우),\n","`Model` 클래스를 서브클래싱하고,\n","`fit()` 동안 반복적으로 호출되는 `train_step()` 메서드를 직접 구현할 수 있습니다.\n","\n","이제, 트레이닝 및 평가를 아주 낮은 레벨에서 제어하려면,\n","처음부터 직접 트레이닝 및 평가 루프를 작성해야 합니다.\n","이 가이드는 바로 이러한 방법에 대해 설명합니다."],"metadata":{"id":"o7QG9NgpLmai"}},{"cell_type":"markdown","source":["## 첫 번째 엔드 투 엔드 예제"],"metadata":{"id":"yu0rVB3HLrzW"}},{"cell_type":"markdown","source":["커스텀 트레이닝 루프를 작성하려면, 다음과 같은 요소들이 필요합니다:\n","\n","-   물론, 트레이닝할 모델.\n","-   옵티마이저.\n","    - `keras.optimizers`의 옵티마이저나 `torch.optim`의 파이토치 네이티브 옵티마이저 중 하나를 사용할 수 있습니다.\n","-   손실 함수.\n","    -   `keras.losses`의 손실 함수나 `torch.nn`의 파이토치 네이티브 손실 함수를 사용할 수 있습니다.\n","-   데이터셋.\n","    -   [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), 파이토치 `DataLoader`, 파이썬 생성기 등 어떤 형식도 사용할 수 있습니다.\n","\n","이 요소들을 정리해보겠습니다.\n","각 경우에 대해 파이토치 네이티브 객체를 사용할 것입니다 — 단, 물론 Keras 모델은 제외하고요.\n","\n","먼저, 모델과 MNIST 데이터셋을 가져와 보겠습니다:"],"metadata":{"id":"D70NVGy9Lto4"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"GVYK8YO3LIc5","executionInfo":{"status":"ok","timestamp":1726966256973,"user_tz":-540,"elapsed":187,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["# 간단한 MNIST 모델을 고려해 봅시다\n","def get_model():\n","    inputs = keras.Input(shape=(784,), name=\"digits\")\n","    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n","    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n","    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n","    model = keras.Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","\n","# MNIST 데이터셋을 로드하고 이를 파이토치 DataLoader에 넣습니다\n","# 트레이닝 데이터셋을 준비합니다.\n","batch_size = 32\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\")\n","x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\")\n","y_train = keras.utils.to_categorical(y_train)\n","y_test = keras.utils.to_categorical(y_test)\n","\n","# 10,000개의 샘플을 검증용으로 예약합니다.\n","x_val = x_train[-10000:]\n","y_val = y_train[-10000:]\n","x_train = x_train[:-10000]\n","y_train = y_train[:-10000]\n","\n","# 파이토치 Datasets 생성\n","train_dataset = torch.utils.data.TensorDataset(\n","    torch.from_numpy(x_train), torch.from_numpy(y_train)\n",")\n","val_dataset = torch.utils.data.TensorDataset(\n","    torch.from_numpy(x_val), torch.from_numpy(y_val)\n",")\n","\n","# DataLoaders 생성\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=batch_size, shuffle=True\n",")\n","val_dataloader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=batch_size, shuffle=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"8fyiE9jULIc6"},"source":["다음은 PyTorch 옵티마이저와 PyTorch 손실 함수입니다:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5VUdMZRgLIc6","executionInfo":{"status":"ok","timestamp":1726966256976,"user_tz":-540,"elapsed":2,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["# # 파이토치 옵티마이저 인스턴스화\n","# model = get_model()\n","# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# # 파이토치 손실 함수 인스턴스화\n","# loss_fn = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","source":["# 파이토치 옵티마이저 인스턴스화\n","model = get_model()\n","model.to('cuda') # Move model to GPU\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# 파이토치 손실 함수 인스턴스화\n","loss_fn = torch.nn.CrossEntropyLoss()"],"metadata":{"id":"fcNCOfIOMFbv","executionInfo":{"status":"ok","timestamp":1726966257077,"user_tz":-540,"elapsed":101,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4KT4IHFLIc7"},"source":["이제 커스텀 트레이닝 루프를 사용하여, 미니 배치 그래디언트로 모델을 트레이닝해봅시다.\n","\n","손실 텐서에 대해 `loss.backward()`를 호출하면 역전파가 실행됩니다.\n","이렇게 하면, 옵티마이저는 각 변수에 대한 그래디언트를 자동으로 알 수 있고,\n","이를 통해 `optimizer.step()`를 사용하여 변수들을 업데이트할 수 있습니다.\n","텐서, 변수, 옵티마이저는 모두 숨겨진 전역 상태를 통해 서로 연결되어 있습니다.\n","또한 `loss.backward()`를 호출하기 전에,\n","`model.zero_grad()`를 호출하는 것을 잊지 마세요.\n","그렇지 않으면, 변수에 대한 올바른 그래디언트를 얻을 수 없습니다.\n","\n","다음은 단계별로 설명한, 트레이닝 루프입니다:\n","\n","-   에포크를 반복하는 `for` 루프를 엽니다.\n","-   각 에포크마다, 데이터셋을 배치 단위로 반복하는 `for` 루프를 엽니다.\n","-   각 배치마다, 입력 데이터를 모델에 전달하여 예측값을 얻은 다음, 이를 사용해 손실 값을 계산합니다.\n","-   `loss.backward()`를 호출합니다.\n","-   루프 scope에서, 손실에 대한 모델 가중치의 그래디언트를 가져옵니다.\n","-   마지막으로, 옵티마이저를 사용하여 그래디언트를 기반으로 모델의 가중치를 업데이트합니다."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"QQ0s1LEDLIc7","executionInfo":{"status":"ok","timestamp":1726966257083,"user_tz":-540,"elapsed":2,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["# epochs = 3\n","# for epoch in range(epochs):\n","#     for step, (inputs, targets) in enumerate(train_dataloader):\n","#         # 순전파 (Forward pass)\n","#         logits = model(inputs)\n","#         loss = loss_fn(logits, targets)\n","\n","#         # 역전파 (Backward pass)\n","#         model.zero_grad()\n","#         loss.backward()\n","\n","#         # 옵티마이저 변수 업데이트\n","#         optimizer.step()\n","\n","#         # 100개 배치마다 로그 출력\n","#         if step % 100 == 0:\n","#             print(\n","#                 f\"트레이닝 손실 (1개의 배치당) - 스텝 {step}: {loss.detach().numpy():.4f}\"\n","#             )\n","#             print(f\"지금까지 처리한 샘플 수: {(step + 1) * batch_size}\")"]},{"cell_type":"code","source":["epochs = 3\n","for epoch in range(epochs):\n","    for step, (inputs, targets) in enumerate(train_dataloader):\n","        # Move inputs and targets to the GPU\n","        inputs = inputs.to('cuda')\n","        targets = targets.to('cuda')\n","\n","        # 순전파 (Forward pass)\n","        logits = model(inputs)\n","        loss = loss_fn(logits, targets)\n","\n","        # 역전파 (Backward pass)\n","        model.zero_grad()\n","        loss.backward()\n","\n","        # 옵티마이저 변수 업데이트\n","        optimizer.step()\n","\n","        # 100개 배치마다 로그 출력\n","        if step % 100 == 0:\n","            print(\n","                f\"트레이닝 손실 (1개의 배치당) - 스텝 {step}: {loss.cpu().detach().numpy():.4f}\" # Move loss tensor to CPU before converting to NumPy\n","            )\n","            print(f\"지금까지 처리한 샘플 수: {(step + 1) * batch_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qLKf1fUMRCu","executionInfo":{"status":"ok","timestamp":1726966265295,"user_tz":-540,"elapsed":8211,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"af879740-af34-4186-f67c-25345ef18259"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["트레이닝 손실 (1개의 배치당) - 스텝 0: 99.3773\n","지금까지 처리한 샘플 수: 32\n","트레이닝 손실 (1개의 배치당) - 스텝 100: 5.1677\n","지금까지 처리한 샘플 수: 3232\n","트레이닝 손실 (1개의 배치당) - 스텝 200: 2.4773\n","지금까지 처리한 샘플 수: 6432\n","트레이닝 손실 (1개의 배치당) - 스텝 300: 3.8496\n","지금까지 처리한 샘플 수: 9632\n","트레이닝 손실 (1개의 배치당) - 스텝 400: 2.1209\n","지금까지 처리한 샘플 수: 12832\n","트레이닝 손실 (1개의 배치당) - 스텝 500: 1.8864\n","지금까지 처리한 샘플 수: 16032\n","트레이닝 손실 (1개의 배치당) - 스텝 600: 1.0746\n","지금까지 처리한 샘플 수: 19232\n","트레이닝 손실 (1개의 배치당) - 스텝 700: 0.3781\n","지금까지 처리한 샘플 수: 22432\n","트레이닝 손실 (1개의 배치당) - 스텝 800: 1.3570\n","지금까지 처리한 샘플 수: 25632\n","트레이닝 손실 (1개의 배치당) - 스텝 900: 0.5862\n","지금까지 처리한 샘플 수: 28832\n","트레이닝 손실 (1개의 배치당) - 스텝 1000: 0.8429\n","지금까지 처리한 샘플 수: 32032\n","트레이닝 손실 (1개의 배치당) - 스텝 1100: 1.0655\n","지금까지 처리한 샘플 수: 35232\n","트레이닝 손실 (1개의 배치당) - 스텝 1200: 0.2928\n","지금까지 처리한 샘플 수: 38432\n","트레이닝 손실 (1개의 배치당) - 스텝 1300: 0.4490\n","지금까지 처리한 샘플 수: 41632\n","트레이닝 손실 (1개의 배치당) - 스텝 1400: 0.1401\n","지금까지 처리한 샘플 수: 44832\n","트레이닝 손실 (1개의 배치당) - 스텝 1500: 0.5578\n","지금까지 처리한 샘플 수: 48032\n","트레이닝 손실 (1개의 배치당) - 스텝 0: 0.4138\n","지금까지 처리한 샘플 수: 32\n","트레이닝 손실 (1개의 배치당) - 스텝 100: 1.3891\n","지금까지 처리한 샘플 수: 3232\n","트레이닝 손실 (1개의 배치당) - 스텝 200: 0.3384\n","지금까지 처리한 샘플 수: 6432\n","트레이닝 손실 (1개의 배치당) - 스텝 300: 0.2574\n","지금까지 처리한 샘플 수: 9632\n","트레이닝 손실 (1개의 배치당) - 스텝 400: 0.9040\n","지금까지 처리한 샘플 수: 12832\n","트레이닝 손실 (1개의 배치당) - 스텝 500: 0.0866\n","지금까지 처리한 샘플 수: 16032\n","트레이닝 손실 (1개의 배치당) - 스텝 600: 0.2198\n","지금까지 처리한 샘플 수: 19232\n","트레이닝 손실 (1개의 배치당) - 스텝 700: 0.3064\n","지금까지 처리한 샘플 수: 22432\n","트레이닝 손실 (1개의 배치당) - 스텝 800: 0.1223\n","지금까지 처리한 샘플 수: 25632\n","트레이닝 손실 (1개의 배치당) - 스텝 900: 0.7231\n","지금까지 처리한 샘플 수: 28832\n","트레이닝 손실 (1개의 배치당) - 스텝 1000: 0.2857\n","지금까지 처리한 샘플 수: 32032\n","트레이닝 손실 (1개의 배치당) - 스텝 1100: 1.0776\n","지금까지 처리한 샘플 수: 35232\n","트레이닝 손실 (1개의 배치당) - 스텝 1200: 0.4159\n","지금까지 처리한 샘플 수: 38432\n","트레이닝 손실 (1개의 배치당) - 스텝 1300: 0.6762\n","지금까지 처리한 샘플 수: 41632\n","트레이닝 손실 (1개의 배치당) - 스텝 1400: 0.1101\n","지금까지 처리한 샘플 수: 44832\n","트레이닝 손실 (1개의 배치당) - 스텝 1500: 0.3615\n","지금까지 처리한 샘플 수: 48032\n","트레이닝 손실 (1개의 배치당) - 스텝 0: 0.0372\n","지금까지 처리한 샘플 수: 32\n","트레이닝 손실 (1개의 배치당) - 스텝 100: 0.7084\n","지금까지 처리한 샘플 수: 3232\n","트레이닝 손실 (1개의 배치당) - 스텝 200: 0.2627\n","지금까지 처리한 샘플 수: 6432\n","트레이닝 손실 (1개의 배치당) - 스텝 300: 0.0259\n","지금까지 처리한 샘플 수: 9632\n","트레이닝 손실 (1개의 배치당) - 스텝 400: 0.2815\n","지금까지 처리한 샘플 수: 12832\n","트레이닝 손실 (1개의 배치당) - 스텝 500: 0.1948\n","지금까지 처리한 샘플 수: 16032\n","트레이닝 손실 (1개의 배치당) - 스텝 600: 0.0810\n","지금까지 처리한 샘플 수: 19232\n","트레이닝 손실 (1개의 배치당) - 스텝 700: 1.2098\n","지금까지 처리한 샘플 수: 22432\n","트레이닝 손실 (1개의 배치당) - 스텝 800: 0.3805\n","지금까지 처리한 샘플 수: 25632\n","트레이닝 손실 (1개의 배치당) - 스텝 900: 0.6068\n","지금까지 처리한 샘플 수: 28832\n","트레이닝 손실 (1개의 배치당) - 스텝 1000: 0.2110\n","지금까지 처리한 샘플 수: 32032\n","트레이닝 손실 (1개의 배치당) - 스텝 1100: 0.3940\n","지금까지 처리한 샘플 수: 35232\n","트레이닝 손실 (1개의 배치당) - 스텝 1200: 0.1379\n","지금까지 처리한 샘플 수: 38432\n","트레이닝 손실 (1개의 배치당) - 스텝 1300: 0.2304\n","지금까지 처리한 샘플 수: 41632\n","트레이닝 손실 (1개의 배치당) - 스텝 1400: 0.2257\n","지금까지 처리한 샘플 수: 44832\n","트레이닝 손실 (1개의 배치당) - 스텝 1500: 0.4296\n","지금까지 처리한 샘플 수: 48032\n"]}]},{"cell_type":"markdown","metadata":{"id":"Z__JcEg-LIc7"},"source":["다른 방법으로, Keras 옵티마이저와 Keras 손실 함수를 사용할 때의 루프가 어떻게 보이는지 살펴보겠습니다.\n","\n","중요한 차이점:\n","\n","- 각 트레이너블 변수에 대해 호출되는, `v.value.grad`를 통해 변수의 그래디언트를 가져옵니다.\n","- `optimizer.apply()`를 통해 변수들을 업데이트하는데, 이는 반드시 `torch.no_grad()` scope 내에서 호출되어야 합니다.\n","\n","**또한 중요한 주의사항:** NumPy/TensorFlow/JAX/Keras API뿐만 아니라 Python의 `unittest` API는 모두,\n","`fn(y_true, y_pred)` (참고값이 먼저, 예측값이 두 번째) 순서의 인자 순서를 사용하지만,\n","PyTorch는 실제로 손실에 대해 `fn(y_pred, y_true)` 순서를 사용합니다.\n","따라서, `logits`와 `targets`의 순서를 바꿔야 합니다."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"TjJO6KgQLIc7","executionInfo":{"status":"ok","timestamp":1726966265307,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["# model = get_model()\n","# optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n","# loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n","\n","# for epoch in range(epochs):\n","#     print(f\"\\n에포크 {epoch} 시작\")\n","#     for step, (inputs, targets) in enumerate(train_dataloader):\n","#         # 순전파 (Forward pass)\n","#         logits = model(inputs)\n","#         loss = loss_fn(targets, logits)\n","\n","#         # 역전파 (Backward pass)\n","#         model.zero_grad()\n","#         trainable_weights = [v for v in model.trainable_weights]\n","\n","#         # 손실에 대해 torch.Tensor.backward()를 호출하여\n","#         # 가중치에 대한 그래디언트를 계산합니다.\n","#         loss.backward()\n","#         gradients = [v.value.grad for v in trainable_weights]\n","\n","#         # 가중치 업데이트\n","#         with torch.no_grad():\n","#             optimizer.apply(gradients, trainable_weights)\n","\n","#         # 100개 배치마다 로그 출력\n","#         if step % 100 == 0:\n","#             print(\n","#                 f\"트레이닝 손실 (1개의 배치당) - 스텝 {step}: {loss.detach().numpy():.4f}\"\n","#             )\n","#             print(f\"지금까지 처리한 샘플 수: {(step + 1) * batch_size}\")"]},{"cell_type":"code","source":["model = get_model()\n","optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n","loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n","\n","for epoch in range(epochs):\n","    print(f\"\\n에포크 {epoch} 시작\")\n","    for step, (inputs, targets) in enumerate(train_dataloader):\n","        # 순전파 (Forward pass)\n","        logits = model(inputs)\n","        loss = loss_fn(targets, logits)\n","\n","        # 역전파 (Backward pass)\n","        model.zero_grad()\n","        trainable_weights = [v for v in model.trainable_weights]\n","\n","        # 손실에 대해 torch.Tensor.backward()를 호출하여\n","        # 가중치에 대한 그래디언트를 계산합니다.\n","        loss.backward()\n","        gradients = [v.value.grad for v in trainable_weights]\n","\n","        # 가중치 업데이트\n","        with torch.no_grad():\n","            optimizer.apply(gradients, trainable_weights)\n","\n","        # 100개 배치마다 로그 출력\n","        if step % 100 == 0:\n","            print(\n","                f\"트레이닝 손실 (1개의 배치당) - 스텝 {step}: {loss.cpu().detach().numpy():.4f}\" # Move loss tensor to CPU before converting to NumPy\n","            )\n","            print(f\"지금까지 처리한 샘플 수: {(step + 1) * batch_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_9vsC7uMpL2","outputId":"bee896ed-01cc-4aef-9582-57be5ef8429c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","에포크 0 시작\n","트레이닝 손실 (1개의 배치당) - 스텝 0: 124.1824\n","지금까지 처리한 샘플 수: 32\n","트레이닝 손실 (1개의 배치당) - 스텝 100: 2.7788\n","지금까지 처리한 샘플 수: 3232\n","트레이닝 손실 (1개의 배치당) - 스텝 200: 2.5986\n","지금까지 처리한 샘플 수: 6432\n","트레이닝 손실 (1개의 배치당) - 스텝 300: 0.6358\n","지금까지 처리한 샘플 수: 9632\n","트레이닝 손실 (1개의 배치당) - 스텝 400: 1.1681\n","지금까지 처리한 샘플 수: 12832\n","트레이닝 손실 (1개의 배치당) - 스텝 500: 0.7495\n","지금까지 처리한 샘플 수: 16032\n","트레이닝 손실 (1개의 배치당) - 스텝 600: 0.7478\n","지금까지 처리한 샘플 수: 19232\n","트레이닝 손실 (1개의 배치당) - 스텝 700: 0.6152\n","지금까지 처리한 샘플 수: 22432\n","트레이닝 손실 (1개의 배치당) - 스텝 800: 0.7670\n","지금까지 처리한 샘플 수: 25632\n","트레이닝 손실 (1개의 배치당) - 스텝 900: 1.0986\n","지금까지 처리한 샘플 수: 28832\n","트레이닝 손실 (1개의 배치당) - 스텝 1000: 0.7430\n","지금까지 처리한 샘플 수: 32032\n","트레이닝 손실 (1개의 배치당) - 스텝 1100: 0.5902\n","지금까지 처리한 샘플 수: 35232\n","트레이닝 손실 (1개의 배치당) - 스텝 1200: 0.4721\n","지금까지 처리한 샘플 수: 38432\n","트레이닝 손실 (1개의 배치당) - 스텝 1300: 0.5501\n","지금까지 처리한 샘플 수: 41632\n","트레이닝 손실 (1개의 배치당) - 스텝 1400: 0.5886\n","지금까지 처리한 샘플 수: 44832\n","트레이닝 손실 (1개의 배치당) - 스텝 1500: 0.4380\n","지금까지 처리한 샘플 수: 48032\n","\n","에포크 1 시작\n","트레이닝 손실 (1개의 배치당) - 스텝 0: 0.8272\n","지금까지 처리한 샘플 수: 32\n","트레이닝 손실 (1개의 배치당) - 스텝 100: 0.1102\n","지금까지 처리한 샘플 수: 3232\n","트레이닝 손실 (1개의 배치당) - 스텝 200: 0.2357\n","지금까지 처리한 샘플 수: 6432\n","트레이닝 손실 (1개의 배치당) - 스텝 300: 0.2710\n","지금까지 처리한 샘플 수: 9632\n","트레이닝 손실 (1개의 배치당) - 스텝 400: 0.6532\n","지금까지 처리한 샘플 수: 12832\n","트레이닝 손실 (1개의 배치당) - 스텝 500: 0.2414\n","지금까지 처리한 샘플 수: 16032\n"]}]},{"cell_type":"markdown","source":["## 메트릭의 낮은 레벨 처리"],"metadata":{"id":"4R_4Q6SbM0OF"}},{"cell_type":"markdown","source":["이 기본 트레이닝 루프에 메트릭 모니터링을 추가해봅시다.\n","\n","이와 같이 처음부터 작성한 트레이닝 루프에서,\n","Keras의 빌트인 메트릭(또는 직접 작성한 커스텀 메트릭)을 쉽게 재사용할 수 있습니다.\n","흐름은 다음과 같습니다:\n","\n","- 루프 시작 시 메트릭 인스턴스화\n","- 각 배치 후 `metric.update_state()` 호출\n","- 메트릭의 현재 값을 표시해야 할 때, `metric.result()` 호출\n","- 메트릭의 상태를 초기화해야 할 때(일반적으로 에포크가 끝날 때), `metric.reset_state()` 호출\n","\n","이 지식을 사용하여 각 에포크가 끝날 때,\n","트레이닝 및 검증 데이터에 대해 `CategoricalAccuracy`를 계산해봅시다:"],"metadata":{"id":"o1fJH4GMM2D1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3nSpnrKLIc7"},"outputs":[],"source":["# 새로운 모델 가져오기\n","model = get_model()\n","\n","# 모델을 트레이닝할 옵티마이저 인스턴스화.\n","optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n","# 손실 함수 인스턴스화.\n","loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n","\n","# 메트릭 준비.\n","train_acc_metric = keras.metrics.CategoricalAccuracy()\n","val_acc_metric = keras.metrics.CategoricalAccuracy()"]},{"cell_type":"markdown","metadata":{"id":"yx_c1XQ4LIc7"},"source":["다음은 우리의 트레이닝 및 평가 루프입니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDYFNMnqLIc8"},"outputs":[],"source":["# for epoch in range(epochs):\n","#     print(f\"\\n에포크 {epoch} 시작\")\n","#     for step, (inputs, targets) in enumerate(train_dataloader):\n","#         # 순전파 (Forward pass)\n","#         logits = model(inputs)\n","#         loss = loss_fn(targets, logits)\n","\n","#         # 역전파 (Backward pass)\n","#         model.zero_grad()\n","#         trainable_weights = [v for v in model.trainable_weights]\n","\n","#         # 손실에 대해 torch.Tensor.backward()를 호출하여\n","#         # 가중치에 대한 그래디언트를 계산합니다.\n","#         loss.backward()\n","#         gradients = [v.value.grad for v in trainable_weights]\n","\n","#         # 가중치 업데이트\n","#         with torch.no_grad():\n","#             optimizer.apply(gradients, trainable_weights)\n","\n","#         # 트레이닝 메트릭 업데이트\n","#         train_acc_metric.update_state(targets, logits)\n","\n","#         # 100개 배치마다 로그 출력\n","#         if step % 100 == 0:\n","#             print(\n","#                 f\"트레이닝 손실 (1개의 배치당) - 스텝 {step}: {loss.detach().numpy():.4f}\"\n","#             )\n","#             print(f\"지금까지 처리한 샘플 수: {(step + 1) * batch_size}\")\n","\n","#     # 각 에포크가 끝날 때 메트릭 표시\n","#     train_acc = train_acc_metric.result()\n","#     print(f\"에포크 동안의 트레이닝 정확도: {float(train_acc):.4f}\")\n","\n","#     # 각 에포크가 끝날 때 트레이닝 메트릭 초기화\n","#     train_acc_metric.reset_state()\n","\n","#     # 각 에포크가 끝날 때 검증 루프 실행\n","#     for x_batch_val, y_batch_val in val_dataloader:\n","#         val_logits = model(x_batch_val, training=False)\n","#         # 검증 메트릭 업데이트\n","#         val_acc_metric.update_state(y_batch_val, val_logits)\n","#     val_acc = val_acc_metric.result()\n","#     val_acc_metric.reset_state()\n","#     print(f\"검증 정확도: {float(val_acc):.4f}\")"]},{"cell_type":"code","source":["for epoch in range(epochs):\n","    print(f\"\\n에포크 {epoch} 시작\")\n","    for step, (inputs, targets) in enumerate(train_dataloader):\n","        # 순전파 (Forward pass)\n","        logits = model(inputs)\n","        loss = loss_fn(targets, logits)\n","\n","        # 역전파 (Backward pass)\n","        model.zero_grad()\n","        trainable_weights = [v for v in model.trainable_weights]\n","\n","        # 손실에 대해 torch.Tensor.backward()를 호출하여\n","        # 가중치에 대한 그래디언트를 계산합니다.\n","        loss.backward()\n","        gradients = [v.value.grad for v in trainable_weights]\n","\n","        # 가중치 업데이트\n","        with torch.no_grad():\n","            optimizer.apply(gradients, trainable_weights)\n","\n","        # 트레이닝 메트릭 업데이트\n","        train_acc_metric.update_state(targets, logits)\n","\n","        # 100개 배치마다 로그 출력\n","        if step % 100 == 0:\n","            print(\n","                f\"트레이닝 손실 (1개의 배치당) - 스텝 {step}: {loss.cpu().detach().numpy():.4f}\" # Move loss tensor to CPU before converting to NumPy\n","            )\n","            print(f\"지금까지 처리한 샘플 수: {(step + 1) * batch_size}\")\n","\n","    # 각 에포크가 끝날 때 메트릭 표시\n","    train_acc = train_acc_metric.result()\n","    print(f\"에포크 동안의 트레이닝 정확도: {float(train_acc):.4f}\")\n","\n","    # 각 에포크가 끝날 때 트레이닝 메트릭 초기화\n","    train_acc_metric.reset_state()\n","\n","    # 각 에포크가 끝날 때 검증 루프 실행\n","    for x_batch_val, y_batch_val in val_dataloader:\n","        val_logits = model(x_batch_val, training=False)\n","        # 검증 메트릭 업데이트\n","        val_acc_metric.update_state(y_batch_val, val_logits)\n","    val_acc = val_acc_metric.result()\n","    val_acc_metric.reset_state()\n","    print(f\"검증 정확도: {float(val_acc):.4f}\")"],"metadata":{"id":"bzwvOoHgM9y8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델에 의해 추적되는 손실의 낮은 레벨 처리"],"metadata":{"id":"jnTCoZLINH47"}},{"cell_type":"markdown","source":["레이어와 모델은 순전파 중 `self.add_loss(value)`를 호출하는 레이어에 의해 생성된 모든 손실을 재귀적으로 추적합니다.\n","이렇게 생성된 스칼라 손실 값 목록은 순전파가 끝날 때 `model.losses` 속성을 통해 확인할 수 있습니다.\n","\n","이러한 손실 구성 요소를 사용하려면, 이를 합산하여 트레이닝 스텝에서 메인 손실에 추가해야 합니다.\n","\n","다음은 activity 정규화 손실을 생성하는 레이어를 고려한 예제입니다:"],"metadata":{"id":"6r1WMCsvNJc2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQ2qGzYuLIc8"},"outputs":[],"source":["class ActivityRegularizationLayer(keras.layers.Layer):\n","    def call(self, inputs):\n","        self.add_loss(1e-2 * torch.sum(inputs))\n","        return inputs"]},{"cell_type":"markdown","metadata":{"id":"E7Wl1YP4LIc8"},"source":["이 레이어를 사용하는 정말 간단한 모델을 만들어 봅시다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1y1mQ3RALIc8"},"outputs":[],"source":["inputs = keras.Input(shape=(784,), name=\"digits\")\n","x = keras.layers.Dense(64, activation=\"relu\")(inputs)\n","# 레이어로 activity 정규화 삽입\n","x = ActivityRegularizationLayer()(x)\n","x = keras.layers.Dense(64, activation=\"relu\")(x)\n","outputs = keras.layers.Dense(10, name=\"predictions\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"markdown","metadata":{"id":"ZS2dCzAxLIc8"},"source":["이제 우리의 트레이닝 루프는 다음과 같이 되어야 합니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zNDyheJLIc8"},"outputs":[],"source":["# # 새로운 모델 가져오기\n","# model = get_model()\n","\n","# # 모델을 트레이닝할 옵티마이저 인스턴스화.\n","# optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n","# # 손실 함수 인스턴스화.\n","# loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n","\n","# # 메트릭 준비.\n","# train_acc_metric = keras.metrics.CategoricalAccuracy()\n","# val_acc_metric = keras.metrics.CategoricalAccuracy()\n","\n","# for epoch in range(epochs):\n","#     print(f\"\\n에포크 {epoch} 시작\")\n","#     for step, (inputs, targets) in enumerate(train_dataloader):\n","#         # 순전파 (Forward pass)\n","#         logits = model(inputs)\n","#         loss = loss_fn(targets, logits)\n","#         if model.losses:\n","#             loss = loss + torch.sum(*model.losses)\n","\n","#         # 역전파 (Backward pass)\n","#         model.zero_grad()\n","#         trainable_weights = [v for v in model.trainable_weights]\n","\n","#         # 손실에 대해 torch.Tensor.backward()를 호출하여\n","#         # 가중치에 대한 그래디언트를 계산합니다.\n","#         loss.backward()\n","#         gradients = [v.value.grad for v in trainable_weights]\n","\n","#         # 가중치 업데이트\n","#         with torch.no_grad():\n","#             optimizer.apply(gradients, trainable_weights)\n","\n","#         # 트레이닝 메트릭 업데이트\n","#         train_acc_metric.update_state(targets, logits)\n","\n","#         # 100개 배치마다 로그 출력\n","#         if step % 100 == 0:\n","#             print(\n","#                 f\"트레이닝 손실 (1개의 배치당) - 스텝 {step}: {loss.detach().numpy():.4f}\"\n","#             )\n","#             print(f\"지금까지 처리한 샘플 수: {(step + 1) * batch_size}\")\n","\n","#     # 각 에포크가 끝날 때 메트릭 표시\n","#     train_acc = train_acc_metric.result()\n","#     print(f\"에포크 동안의 트레이닝 정확도: {float(train_acc):.4f}\")\n","\n","#     # 각 에포크가 끝날 때 트레이닝 메트릭 초기화\n","#     train_acc_metric.reset_state()\n","\n","#     # 각 에포크가 끝날 때 검증 루프 실행\n","#     for x_batch_val, y_batch_val in val_dataloader:\n","#         val_logits = model(x_batch_val, training=False)\n","#         # 검증 메트릭 업데이트\n","#         val_acc_metric.update_state(y_batch_val, val_logits)\n","#     val_acc = val_acc_metric.result()\n","#     val_acc_metric.reset_state()\n","#     print(f\"검증 정확도: {float(val_acc):.4f}\")"]},{"cell_type":"code","source":["# 새로운 모델 가져오기\n","model = get_model()\n","\n","# 모델을 트레이닝할 옵티마이저 인스턴스화.\n","optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n","# 손실 함수 인스턴스화.\n","loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n","\n","# 메트릭 준비.\n","train_acc_metric = keras.metrics.CategoricalAccuracy()\n","val_acc_metric = keras.metrics.CategoricalAccuracy()\n","\n","for epoch in range(epochs):\n","    print(f\"\\n에포크 {epoch} 시작\")\n","    for step, (inputs, targets) in enumerate(train_dataloader):\n","        # 순전파 (Forward pass)\n","        logits = model(inputs)\n","        loss = loss_fn(targets, logits)\n","        if model.losses:\n","            loss = loss + torch.sum(*model.losses)\n","\n","        # 역전파 (Backward pass)\n","        model.zero_grad()\n","        trainable_weights = [v for v in model.trainable_weights]\n","\n","        # 손실에 대해 torch.Tensor.backward()를 호출하여\n","        # 가중치에 대한 그래디언트를 계산합니다.\n","        loss.backward()\n","        gradients = [v.value.grad for v in trainable_weights]\n","\n","        # 가중치 업데이트\n","        with torch.no_grad():\n","            optimizer.apply(gradients, trainable_weights)\n","\n","        # 트레이닝 메트릭 업데이트\n","        train_acc_metric.update_state(targets, logits)\n","\n","        # 100개 배치마다 로그 출력\n","        if step % 100 == 0:\n","            print(\n","                f\"트레이닝 손실 (1개의 배치당) - 스텝 {step}: {loss.cpu().detach().numpy():.4f}\" # Move loss tensor to CPU before converting to NumPy\n","            )\n","            print(f\"지금까지 처리한 샘플 수: {(step + 1) * batch_size}\")\n","\n","    # 각 에포크가 끝날 때 메트릭 표시\n","    train_acc = train_acc_metric.result()\n","    print(f\"에포크 동안의 트레이닝 정확도: {float(train_acc):.4f}\")\n","\n","    # 각 에포크가 끝날 때 트레이닝 메트릭 초기화\n","    train_acc_metric.reset_state()\n","\n","    # 각 에포크가 끝날 때 검증 루프 실행\n","    for x_batch_val, y_batch_val in val_dataloader:\n","        val_logits = model(x_batch_val, training=False)\n","        # 검증 메트릭 업데이트\n","        val_acc_metric.update_state(y_batch_val, val_logits)\n","    val_acc = val_acc_metric.result()\n","    val_acc_metric.reset_state()\n","    print(f\"검증 정확도: {float(val_acc):.4f}\")"],"metadata":{"id":"bLLQ-QrJNUfW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ps3NXM9eLIc8"},"source":["이것으로 끝입니다!"]}],"metadata":{"accelerator":"None","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/guides/ipynb/writing_a_custom_training_loop_in_torch.ipynb","timestamp":1726965651859}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}