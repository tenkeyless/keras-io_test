{"cells":[{"cell_type":"markdown","source":["# PyTorch를 사용한 멀티 GPU 분산 트레이닝"],"metadata":{"id":"dz5zsSaTB2d1"}},{"cell_type":"markdown","source":["**저자:** [fchollet](https://twitter.com/fchollet)  \n","**생성일:** 2023/06/29  \n","**최종편집일:** 2023/06/29  \n","**설명:** PyTorch로 Keras 모델을 사용하여, 멀티 GPU 트레이닝을 진행하는 가이드입니다."],"metadata":{"id":"PdpDQ0e-B4cW"}},{"cell_type":"markdown","source":["## 소개"],"metadata":{"id":"kPSvglFwB55f"}},{"cell_type":"markdown","source":["일반적으로 여러 디바이스에 계산을 분산시키는 방법에는 두 가지가 있습니다:\n","\n","- **데이터 병렬 처리**\n","  - **데이터 병렬 처리**에서는 하나의 모델이 여러 장치나 여러 머신에 복제됩니다.\n","  - 각 장치는 서로 다른 배치의 데이터를 처리한 후, 결과를 병합합니다.\n","  - 이 설정에는 다양한 변형이 있으며, 서로 다른 모델 복제본이 결과를 병합하는 방식이나,\n","    각 배치마다 동기화되는지 여부 등에 차이가 있습니다.\n","- **모델 병렬 처리**\n","  - **모델 병렬 처리**에서는 하나의 모델의 다른 부분이 서로 다른 장치에서 실행되어, 하나의 데이터 배치를 함께 처리합니다.\n","  - 이는 여러 가지 브랜치를 특징으로 하는 자연스럽게 병렬화된 아키텍처를 가진 모델에 가장 적합합니다.\n","\n","이 가이드는 데이터 병렬 처리, 특히 **동기식 데이터 병렬 처리**에 중점을 둡니다.\n","여기서 모델의 서로 다른 복제본은 각 배치를 처리한 후 동기화됩니다.\n","동기화는 모델의 수렴 동작을 단일 장치에서의 트레이닝과 동일하게 유지시킵니다.\n","\n","이 가이드는 PyTorch의 `DistributedDataParallel` 모듈 래퍼를 사용하여,\n","Keras를 여러 GPU(일반적으로 2~16개)에서 트레이닝하는 방법을 가르칩니다.\n","이 설정은 단일 머신에 설치된 여러 GPU를 사용하는 싱글 호스트, 멀티 디바이스 트레이닝으로,\n","연구자들과 소규모 산업 워크플로우에서 가장 일반적으로 사용됩니다.\n","이 과정에서 코드를 최소한으로 변경하여 사용할 수 있습니다."],"metadata":{"id":"VohGOsi8B7Rb"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"JE9fSiLgBhXB","executionInfo":{"status":"ok","timestamp":1726999356914,"user_tz":-540,"elapsed":5,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["# 이 노트북은 Keras 3이 설치되어 있다는 가정 하에 진행됩니다.\n","#\n","# !pip install keras --upgrade --quiet"]},{"cell_type":"markdown","source":["## 셋업"],"metadata":{"id":"E_IU9xSKB-wm"}},{"cell_type":"markdown","source":["먼저, 우리가 트레이닝할 모델을 생성하는 함수와,\n","트레이닝할 데이터셋(MNIST)을 생성하는 함수를 정의해봅시다."],"metadata":{"id":"TU56rtz3CAFf"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"MAOI_2MdBhXC","executionInfo":{"status":"ok","timestamp":1726999358740,"user_tz":-540,"elapsed":1821,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["import os\n","\n","os.environ[\"KERAS_BACKEND\"] = \"torch\"\n","\n","import torch\n","import numpy as np\n","import keras\n","\n","\n","def get_model():\n","    # 배치 정규화와 드롭아웃이 포함된, 간단한 컨브넷(convnet)을 만듭니다.\n","    inputs = keras.Input(shape=(28, 28, 1))\n","    x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n","    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(x)\n","    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n","    x = keras.layers.ReLU()(x)\n","    x = keras.layers.Conv2D(filters=24, kernel_size=6, use_bias=False, strides=2)(x)\n","    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n","    x = keras.layers.ReLU()(x)\n","    x = keras.layers.Conv2D(filters=32, kernel_size=6, padding=\"same\", strides=2, name=\"large_k\")(x)\n","    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n","    x = keras.layers.ReLU()(x)\n","    x = keras.layers.GlobalAveragePooling2D()(x)\n","    x = keras.layers.Dense(256, activation=\"relu\")(x)\n","    x = keras.layers.Dropout(0.5)(x)\n","    outputs = keras.layers.Dense(10)(x)\n","    model = keras.Model(inputs, outputs)\n","    return model\n","\n","\n","def get_dataset():\n","    # 데이터를 불러오고, 트레이닝과 테스트 세트로 나눕니다.\n","    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","\n","    # 이미지를 [0, 1] 범위로 스케일링합니다.\n","    x_train = x_train.astype(\"float32\")\n","    x_test = x_test.astype(\"float32\")\n","    # 이미지가 (28, 28, 1) shape을 가지도록 합니다.\n","    x_train = np.expand_dims(x_train, -1)\n","    x_test = np.expand_dims(x_test, -1)\n","    print(\"x_train shape:\", x_train.shape)\n","\n","    # TensorDataset을 생성합니다.\n","    dataset = torch.utils.data.TensorDataset(\n","        torch.from_numpy(x_train), torch.from_numpy(y_train)\n","    )\n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"ZCatV6OgBhXC"},"source":["이제 GPU를 대상으로 하는 간단한 PyTorch 트레이닝 루프를 정의해보겠습니다. (`.cuda()` 호출에 주목하세요)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"yN3Q2JZqBhXD","executionInfo":{"status":"ok","timestamp":1726999358747,"user_tz":-540,"elapsed":5,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["def train_model(model, dataloader, num_epochs, optimizer, loss_fn):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_loss_count = 0\n","        for batch_idx, (inputs, targets) in enumerate(dataloader):\n","            inputs = inputs.cuda(non_blocking=True)\n","            targets = targets.cuda(non_blocking=True)\n","\n","            # 순방향 패스 (Forward pass)\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, targets)\n","\n","            # 역방향 패스 및 최적화 (Backward and optimize)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            running_loss_count += 1\n","\n","        # 손실 통계 출력\n","        print(\n","            f\"Epoch {epoch + 1}/{num_epochs}, \"\n","            f\"Loss: {running_loss / running_loss_count}\"\n","        )"]},{"cell_type":"markdown","source":["## 단일 호스트, 다중 장치 동기 트레이닝"],"metadata":{"id":"4O5AFwZyCGsN"}},{"cell_type":"markdown","source":["이 설정에서는, 여러 개의 GPU가 있는 하나의 머신(일반적으로 2~16개의 GPU)에서 트레이닝을 진행합니다.\n","각 디바이스는 **복제본(replica)**이라고 불리는 모델의 사본을 실행합니다.\n","간단히 설명하기 위해, 다음 내용에서는 8개의 GPU를 사용하는 것으로 가정하겠습니다. 이는 일반성을 잃지 않습니다.​\n","\n","**작동 방식**\n","\n","트레이닝의 각 단계에서:\n","\n","-   현재 데이터 배치(**글로벌 배치**)는 8개의 서로 다른 하위 배치(**로컬 배치**)로 나뉩니다.\n","    예를 들어, 글로벌 배치에 512개의 샘플이 있으면, 8개의 로컬 배치 각각에는 64개의 샘플이 포함됩니다.\n","-   8개의 복제본 각각은 로컬 배치를 독립적으로 처리합니다:\n","    순전파를 실행한 후, 역전파를 수행하여, 모델 손실에 대한 가중치의 그래디언트를 출력합니다.\n","-   로컬 그래디언트로부터 발생한 가중치 업데이트는 8개의 복제본 간에 효율적으로 병합됩니다.\n","    이 병합은 각 스텝이 끝날 때 이루어지기 때문에, 복제본은 항상 동기화된 상태를 유지합니다.\n","\n","실제로, 모델 레플리카의 가중치를 동기적으로 업데이트하는 과정은 각 개별 가중치 변수 레벨에서 처리됩니다.\n","이는 **미러드 변수(mirrored variable)** 객체를 통해 이루어집니다.\n","\n","**사용 방법**\n","\n","단일 호스트에서 여러 장치로 동기식 트레이닝을 수행하려면, `torch.nn.parallel.DistributedDataParallel` 모듈 래퍼를 사용합니다. 아래는 그 동작 방식입니다:\n","\n","- `torch.multiprocessing.start_processes`를 사용하여 장치별로 하나의 프로세스를 시작합니다.\n","  각 프로세스는 `per_device_launch_fn` 함수를 실행합니다.\n","- `per_device_launch_fn` 함수는 다음과 같은 작업을 수행합니다:\n","  - `torch.distributed.init_process_group`과 `torch.cuda.set_device`를 사용하여,\n","    해당 프로세스에서 사용할 장치를 설정합니다.\n","  - `torch.utils.data.distributed.DistributedSampler`와 `torch.utils.data.DataLoader`를 사용하여,\n","    데이터를 분산 데이터 로더로 변환합니다.\n","  - `torch.nn.parallel.DistributedDataParallel`을 사용하여,\n","    모델을 분산된 PyTorch 모듈로 변환합니다.\n","  - 그런 다음 `train_model` 함수를 호출합니다.\n","- `train_model` 함수는 각 프로세스에서 실행되며, 각 프로세스에서 모델은 별도의 장치를 사용합니다.\n","\n","다음은 각 단계를 유틸리티 함수로 나눈 흐름입니다:"],"metadata":{"id":"BLllgiaiCJu-"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"Cfh2BjYqBhXD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726999358796,"user_tz":-540,"elapsed":48,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"ea3cbd53-8a2f-4e3b-cb54-45d930473325"},"outputs":[{"output_type":"stream","name":"stdout","text":["1개의 GPU에서 실행 중입니다\n"]}],"source":["# 설정\n","num_gpu = torch.cuda.device_count()\n","num_epochs = 2\n","batch_size = 64\n","print(f\"{num_gpu}개의 GPU에서 실행 중입니다\")\n","\n","\n","def setup_device(current_gpu_index, num_gpus):\n","    # 장치 설정\n","    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n","    os.environ[\"MASTER_PORT\"] = \"56492\"\n","    device = torch.device(\"cuda:{}\".format(current_gpu_index))\n","    torch.distributed.init_process_group(\n","        backend=\"nccl\",\n","        init_method=\"env://\",\n","        world_size=num_gpus,\n","        rank=current_gpu_index,\n","    )\n","    torch.cuda.set_device(device)\n","\n","\n","def cleanup():\n","    torch.distributed.destroy_process_group()\n","\n","\n","def prepare_dataloader(dataset, current_gpu_index, num_gpus, batch_size):\n","    sampler = torch.utils.data.distributed.DistributedSampler(\n","        dataset,\n","        num_replicas=num_gpus,\n","        rank=current_gpu_index,\n","        shuffle=False,\n","    )\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        sampler=sampler,\n","        batch_size=batch_size,\n","        shuffle=False,\n","    )\n","    return dataloader\n","\n","\n","def per_device_launch_fn(current_gpu_index, num_gpu):\n","    # 프로세스 그룹 설정\n","    setup_device(current_gpu_index, num_gpu)\n","\n","    dataset = get_dataset()\n","    model = get_model()\n","\n","    # 데이터 로더 준비\n","    dataloader = prepare_dataloader(dataset, current_gpu_index, num_gpu, batch_size)\n","\n","    # torch 최적화 도구 생성\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","    # torch 손실 함수 생성\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    # 모델을 장치에 배치\n","    model = model.to(current_gpu_index)\n","    ddp_model = torch.nn.parallel.DistributedDataParallel(\n","        model, device_ids=[current_gpu_index], output_device=current_gpu_index\n","    )\n","\n","    train_model(ddp_model, dataloader, num_epochs, optimizer, loss_fn)\n","\n","    cleanup()"]},{"cell_type":"markdown","metadata":{"id":"5IXEp3MuBhXD"},"source":["멀티 프로세스를 시작하는 시간입니다:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"DuKAaVCwBhXD","colab":{"base_uri":"https://localhost:8080/","height":688},"executionInfo":{"status":"error","timestamp":1726999555412,"user_tz":-540,"elapsed":50,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"487f86be-4af9-4457-ae2e-a3810fff9dd5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"error","ename":"ProcessRaisedException","evalue":"\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"<ipython-input-4-a95c5cad9f76>\", line 44, in per_device_launch_fn\n    setup_device(current_gpu_index, num_gpu)\n  File \"<ipython-input-4-a95c5cad9f76>\", line 19, in setup_device\n    torch.cuda.set_device(device)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 408, in set_device\n    torch._C._cuda_setDevice(device)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 288, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-8bd1cab1605b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# notebooks을 지원하기 위해, \"spawn\" 대신 \"fork\" 방식을 사용합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 다만, 멀티 GPU 환경이 아니어서 그런가, 아니면 다른 요인으로 인해 에러가 발생.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     torch.multiprocessing.start_processes(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mper_device_launch_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"<ipython-input-4-a95c5cad9f76>\", line 44, in per_device_launch_fn\n    setup_device(current_gpu_index, num_gpu)\n  File \"<ipython-input-4-a95c5cad9f76>\", line 19, in setup_device\n    torch.cuda.set_device(device)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 408, in set_device\n    torch._C._cuda_setDevice(device)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 288, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"]}],"source":["if __name__ == \"__main__\":\n","    # notebooks을 지원하기 위해, \"spawn\" 대신 \"fork\" 방식을 사용합니다.\n","    # 다만, 멀티 GPU 환경이 아니어서 그런가, 아니면 다른 요인으로 인해 에러가 발생.\n","    torch.multiprocessing.start_processes(\n","        per_device_launch_fn,\n","        args=(num_gpu,),\n","        nprocs=num_gpu,\n","        join=True,\n","        start_method=\"fork\",\n","    )"]},{"cell_type":"markdown","metadata":{"id":"VPHbLPqDBhXD"},"source":["이제 끝났습니다!"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/guides/ipynb/distributed_training_with_torch.ipynb","timestamp":1726996686538}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}