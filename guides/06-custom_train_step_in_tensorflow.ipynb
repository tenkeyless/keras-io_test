{"cells":[{"cell_type":"markdown","source":["# TensorFlow에서 `fit()`의 동작을 커스터마이즈하기"],"metadata":{"id":"RuEbcrobzwl9"}},{"cell_type":"markdown","source":["**저자:** [fchollet](https://twitter.com/fchollet)  \n","**생성일:** 2020/04/15  \n","**최종편집일:** 2023/06/27  \n","**설명:** TensorFlow에서 `Model` 클래스의 트레이닝 스텝을 재정의하기\n"],"metadata":{"id":"EYDHoSBbzyY3"}},{"cell_type":"markdown","source":["## 소개"],"metadata":{"id":"Nq0dKG-Jzz1B"}},{"cell_type":"markdown","source":["지도 학습을 할 때는, `fit()`을 사용하면 모든 것이 매끄럽게 작동합니다.\n","\n","모든 세부 사항을 완전히 제어해야 할 경우, 처음부터 끝까지 직접 트레이닝 루프를 작성할 수 있습니다.\n","\n","하지만 커스텀 트레이닝 알고리즘이 필요하면서도,\n","`fit()`의 콜백, 빌트인 분산 지원, 스텝 퓨징(step fusing) 등과 같은,\n","편리한 기능을 그대로 활용하고 싶다면 어떻게 해야 할까요?\n","\n","Keras의 핵심 원칙 중 하나는 **점진적인 복잡성 공개**입니다.\n","항상 점진적으로 더 **낮은 레벨**의 워크플로로 진입할 수 있어야 합니다.\n","**높은 레벨**의 기능이 정확히 사용 사례에 맞지 않더라도,\n","갑작스럽게 어려움에 부딪히지 않아야 합니다.\n","높은 레벨의 편리함을 유지하면서,\n","작은 세부 사항에 대한 제어 권한을 더 많이 가질 수 있어야 합니다.\n","\n","`fit()`의 동작을 커스터마이즈해야 할 때는,\n","**`Model` 클래스의 트레이닝 스텝 함수를 재정의(override)해야** 합니다.\n","이 함수는 `fit()`이 각 데이터 배치마다 호출하는 함수입니다.\n","이렇게 하면 평소와 같이 `fit()`을 호출할 수 있으며,\n","그 안에서 사용자가 정의한 학습 알고리즘이 실행됩니다.\n","\n","이 패턴은 Functional API로 모델을 만드는 것을 방해하지 않는다는 점에 주의하세요.\n","`Sequential` 모델, Functional API 모델, 또는 서브클래싱한 모델을 만들 때도\n","이 방법을 사용할 수 있습니다.\n","\n","이제 그 방법을 살펴보겠습니다."],"metadata":{"id":"UWr2MxxFz293"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"yGn4tMCFzgJY","executionInfo":{"status":"ok","timestamp":1726959612704,"user_tz":-540,"elapsed":3,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["# !pip install keras --upgrade --quiet"]},{"cell_type":"markdown","metadata":{"id":"YUXCSUTMzgJY"},"source":["## 셋업"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Qsg9eK1DzgJY","executionInfo":{"status":"ok","timestamp":1726959614825,"user_tz":-540,"elapsed":1740,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["import os\n","\n","# 이 가이드는 TF 백엔드에서만 실행할 수 있습니다.\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","import tensorflow as tf\n","import keras\n","from keras import layers\n","import numpy as np"]},{"cell_type":"code","source":["from keras import backend\n","print(backend.backend())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EF9a--KVz9x8","executionInfo":{"status":"ok","timestamp":1726959614831,"user_tz":-540,"elapsed":4,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"7d9dce82-5829-4dab-9ef5-a845d943013d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensorflow\n"]}]},{"cell_type":"markdown","source":["## 첫 번째 간단한 예제"],"metadata":{"id":"DvF16vXb0Jh_"}},{"cell_type":"markdown","source":["간단한 예제부터 시작해봅시다:\n","\n","- [`keras.Model`](https://codecompose7.github.io/keras-doc-kr.github.io/api/models/model#model-class)을 서브클래스화하는,\n","  새로운 클래스를 만듭니다.\n","- `train_step(self, data)` 메서드만 재정의합니다.\n","- 메트릭 이름(손실을 포함한)과 현재 값의 매핑을 반환하는 딕셔너리를 리턴합니다.\n","\n","입력 인자 `data`는 `fit`에 트레이닝 데이터로 전달되는 것입니다:\n","\n","- `fit(x, y, ...)`를 호출하면서 NumPy 배열을 전달하면, `data`는 튜플 `(x, y)`가 됩니다.\n","- [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)을 `fit(dataset, ...)`으로 호출하면서 전달하면,\n","  `data`는 각 배치에서 `dataset`이 생성하는 값이 됩니다.\n","\n","`train_step()` 메서드의 본문에서는,\n","이미 익숙한 일반적인 트레이닝 업데이트를 구현합니다.\n","중요한 것은, **`self.compute_loss()`를 통해 손실을 계산**하는 것입니다.\n","이 메서드는 `compile()`에 전달된 손실 함수들을 래핑합니다.\n","\n","마찬가지로, `self.metrics`에서 메트릭에 대해 `metric.update_state(y, y_pred)`를 호출하여,\n","`compile()`에 전달된 메트릭의 상태를 업데이트하고,\n","마지막에는 `self.metrics`에서 결과를 쿼리하여 현재 값을 가져옵니다."],"metadata":{"id":"fyGl23_G0LCb"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"2g_bil3OzgJZ","executionInfo":{"status":"ok","timestamp":1726959649447,"user_tz":-540,"elapsed":44,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["class CustomModel(keras.Model):\n","    def train_step(self, data):\n","        # 데이터를 언패킹합니다. 데이터의 구조는 모델과 `fit()`에 전달하는 값에 따라 다릅니다.\n","        x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # 순전파\n","            # 손실 값을 계산합니다.\n","            # (손실 함수는 `compile()`에서 설정됩니다)\n","            loss = self.compute_loss(y=y, y_pred=y_pred)\n","\n","        # 그래디언트를 계산합니다.\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # 가중치를 업데이트합니다.\n","        self.optimizer.apply(gradients, trainable_vars)\n","\n","        # 메트릭을 업데이트합니다. (손실을 추적하는 메트릭 포함)\n","        for metric in self.metrics:\n","            if metric.name == \"loss\":\n","                metric.update_state(loss)\n","            else:\n","                metric.update_state(y, y_pred)\n","\n","        # 메트릭 이름을 현재 값에 매핑하는 딕셔너리를 반환합니다.\n","        return {m.name: m.result() for m in self.metrics}"]},{"cell_type":"markdown","metadata":{"id":"O2zUVJLGzgJZ"},"source":["이제 이것을 시도해봅시다:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"f73VXL2KzgJZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726959659331,"user_tz":-540,"elapsed":682,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"223b4e14-35a5-4b38-8aa1-37b4a07b9433"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - mae: 0.6351 - loss: 0.5851\n","Epoch 2/3\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - mae: 0.4266 - loss: 0.2831\n","Epoch 3/3\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - mae: 0.4073 - loss: 0.2606\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7f3294da1e70>"]},"metadata":{},"execution_count":5}],"source":["# `CustomModel` 인스턴스를 생성하고 컴파일합니다.\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","\n","# 평소처럼 `fit`을 사용하세요.\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","model.fit(x, y, epochs=3)"]},{"cell_type":"markdown","source":["## 더 낮은 레벨로 내려가기"],"metadata":{"id":"pEEwnudw0Xbk"}},{"cell_type":"markdown","source":["물론, `compile()`에서 손실 함수를 전달하지 않고,\n","대신 모든 것을 `train_step`에서 *수동으로* 수행할 수 있습니다.\n","메트릭도 마찬가지입니다.\n","\n","다음은 옵티마이저를 설정하기 위해서만 `compile()`을 사용하는,\n","더 낮은 레벨의 예제입니다:\n","\n","- 먼저, 손실과 MAE 점수를 추적하기 위해, `Metric` 인스턴스를 생성합니다. (`__init__()`에서)\n","- 그런 다음, 이 메트릭의 상태를 업데이트(메트릭의 `update_state()` 호출)한 후,\n","  현재 평균 값을 반환하기 위해 `result()`를 통해 이를 쿼리하는,\n","  커스텀 `train_step()`을 구현합니다.\n","  이렇게 반환된 값은 진행 표시줄에 표시되거나 콜백에 전달됩니다.\n","- 각 에포크마다 메트릭의 `reset_states()`를 호출해야 한다는 점에 유의하세요!\n","  그렇지 않으면, `result()`를 호출할 때,\n","  에포크 시작 시점이 아닌 트레이닝 시작 이후의 평균을 반환하게 됩니다.\n","  보통 우리는 에포크별 평균을 사용합니다.\n","  다행히도, 프레임워크가 이를 처리해줍니다: 모델의 `metrics` 속성에 초기화하려는 메트릭을 나열하기만 하면 됩니다.\n","  모델은 `fit()` 에포크의 시작 시점이나 `evaluate()` 호출의 시작 시점에,\n","  여기에 나열된 모든 객체에 대해 `reset_states()`를 호출합니다."],"metadata":{"id":"DPQ5Rnfw0ZCW"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"sBNh4oQjzgJZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726959698761,"user_tz":-540,"elapsed":696,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"a37213e2-1306-41dc-c21d-4a716f01938c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3634 - mae: 0.4885\n","Epoch 2/5\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.2231 - mae: 0.3757\n","Epoch 3/5\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.2423 - mae: 0.3920\n","Epoch 4/5\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.2261 - mae: 0.3807\n","Epoch 5/5\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.2037 - mae: 0.3636\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7f328493bcd0>"]},"metadata":{},"execution_count":6}],"source":["class CustomModel(keras.Model):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n","        self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n","        self.loss_fn = keras.losses.MeanSquaredError()\n","\n","    def train_step(self, data):\n","        x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # 순전파\n","            # 우리만의 손실을 계산합니다.\n","            loss = self.loss_fn(y, y_pred)\n","\n","        # 그래디언트를 계산합니다.\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # 가중치를 업데이트합니다.\n","        self.optimizer.apply(gradients, trainable_vars)\n","\n","        # 우리만의 메트릭을 계산합니다.\n","        self.loss_tracker.update_state(loss)\n","        self.mae_metric.update_state(y, y_pred)\n","        return {\n","            \"loss\": self.loss_tracker.result(),\n","            \"mae\": self.mae_metric.result(),\n","        }\n","\n","    @property\n","    def metrics(self):\n","        # 각 에포크의 시작 시점이나 `evaluate()`의 시작 시점에,\n","        # `reset_states()`가 자동으로 호출될 수 있도록,\n","        # `Metric` 객체를 여기에 나열합니다.\n","        return [self.loss_tracker, self.mae_metric]\n","\n","\n","# `CustomModel` 인스턴스를 생성하고 컴파일합니다.\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","\n","# 여기에서는 손실 함수나 메트릭을 전달하지 않습니다.\n","model.compile(optimizer=\"adam\")\n","\n","# 평소처럼 `fit`을 사용하세요 — 콜백 등을 사용할 수 있습니다.\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","model.fit(x, y, epochs=5)"]},{"cell_type":"markdown","source":["## `sample_weight` & `class_weight` 지원"],"metadata":{"id":"9dCzP5Wp0eTH"}},{"cell_type":"markdown","source":["첫 번째 기본 예제에서, 샘플 가중치에 대한 언급이 없었다는 것을 눈치채셨을 겁니다.\n","`sample_weight`와 `class_weight` 같은 `fit()` 인자를 지원하고 싶다면,\n","다음과 같이 간단히 할 수 있습니다:\n","\n","- `data` 인자에서 `sample_weight`를 언패킹합니다.\n","- `compute_loss`와 `update_state`에 이를 전달합니다.\n","  (물론, 손실 및 메트릭에 대해 `compile()`을 사용하지 않는다면,\n","  이를 수동으로 적용할 수도 있습니다)\n","- 그게 전부입니다."],"metadata":{"id":"8629tLUz0f8T"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"Y7sMAD_FzgJa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726959727849,"user_tz":-540,"elapsed":660,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"ae80f905-59f1-4d93-eaee-08fb78a4ce99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - mae: 0.6411 - loss: 0.3055\n","Epoch 2/3\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - mae: 0.4489 - loss: 0.1562\n","Epoch 3/3\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - mae: 0.4074 - loss: 0.1341\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7f3294da1d50>"]},"metadata":{},"execution_count":7}],"source":["class CustomModel(keras.Model):\n","    def train_step(self, data):\n","        # 데이터를 언패킹합니다.\n","        # 데이터의 구조는 모델과 `fit()`에 전달하는 값에 따라 달라집니다.\n","        if len(data) == 3:\n","            x, y, sample_weight = data\n","        else:\n","            sample_weight = None\n","            x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # 순전파\n","            # 손실 값을 계산합니다.\n","            # (손실 함수는 `compile()`에서 설정됩니다)\n","            loss = self.compute_loss(\n","                y=y,\n","                y_pred=y_pred,\n","                sample_weight=sample_weight,\n","            )\n","\n","        # 그래디언트를 계산합니다.\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # 가중치를 업데이트합니다.\n","        self.optimizer.apply(gradients, trainable_vars)\n","\n","        # 메트릭을 업데이트합니다.\n","        # 메트릭은 `compile()`에서 설정됩니다.\n","        for metric in self.metrics:\n","            if metric.name == \"loss\":\n","                metric.update_state(loss)\n","            else:\n","                metric.update_state(y, y_pred, sample_weight=sample_weight)\n","\n","        # 메트릭 이름을 현재 값에 매핑하는 딕셔너리를 반환합니다.\n","        # 여기에는 손실(`self.metrics`에서 추적된)이 포함된다는 점에 유의하세요.\n","        return {m.name: m.result() for m in self.metrics}\n","\n","\n","# `CustomModel` 인스턴스를 생성하고 컴파일합니다.\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","\n","# 이제 `sample_weight` 인자를 사용할 수 있습니다.\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","sw = np.random.random((1000, 1))\n","model.fit(x, y, sample_weight=sw, epochs=3)"]},{"cell_type":"markdown","source":["## 사용자 정의 평가 스텝 제공"],"metadata":{"id":"QZydSSoO0kmb"}},{"cell_type":"markdown","source":["`model.evaluate()` 호출에 대해서도 동일한 작업을 수행하고 싶다면 어떻게 해야 할까요?\n","그러면 정확히 같은 방식으로 `test_step`을 재정의하면 됩니다.\n","예시는 다음과 같습니다:"],"metadata":{"id":"ZtpJu6Q90mBS"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"MG-qXgOizgJa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726959746680,"user_tz":-540,"elapsed":328,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"2c617179-c97d-47b5-899f-47291d45e851"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - mae: 1.2420 - loss: 1.8184\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.7779632806777954, 1.2255314588546753]"]},"metadata":{},"execution_count":8}],"source":["class CustomModel(keras.Model):\n","    def test_step(self, data):\n","        # 데이터를 언패킹합니다.\n","        x, y = data\n","        # 예측값을 계산합니다.\n","        y_pred = self(x, training=False)\n","        # 손실을 추적하는 메트릭을 업데이트합니다.\n","        loss = self.compute_loss(y=y, y_pred=y_pred)\n","        # 메트릭을 업데이트합니다.\n","        for metric in self.metrics:\n","            if metric.name == \"loss\":\n","                metric.update_state(loss)\n","            else:\n","                metric.update_state(y, y_pred)\n","        # 메트릭 이름을 현재 값에 매핑하는 딕셔너리를 반환합니다.\n","        # 여기에는 손실(`self.metrics`에서 추적된)이 포함된다는 점에 유의하세요.\n","        return {m.name: m.result() for m in self.metrics}\n","\n","\n","# `CustomModel` 인스턴스를 생성하고 컴파일합니다.\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(loss=\"mse\", metrics=[\"mae\"])\n","\n","# 커스텀 `test_step`으로 평가합니다.\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","model.evaluate(x, y)"]},{"cell_type":"markdown","source":["## 마무리: 엔드 투 엔드 GAN 예제"],"metadata":{"id":"e2GjN7lw0pLc"}},{"cell_type":"markdown","source":["방금 배운 모든 것을 활용하는 엔드 투 엔드 예제를 함께 살펴보겠습니다.\n","\n","다음의 경우를 고려해봅시다:\n","\n","- 28x28x1 이미지를 생성하는 생성자(generator) 네트워크.\n","- 28x28x1 이미지를 두 개의 클래스(\"가짜\"와 \"진짜\")로 분류하는 판별자(discriminator) 네트워크.\n","- 각 네트워크에 대한 옵티마이저.\n","- 판별자를 트레이닝하기 위한 손실 함수."],"metadata":{"id":"x9m2VblL0qmt"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"zQXPzUSqzgJa","executionInfo":{"status":"ok","timestamp":1726959766575,"user_tz":-540,"elapsed":42,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["# 판별자를 생성합니다.\n","discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape=(28, 28, 1)),\n","        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(negative_slope=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(negative_slope=0.2),\n","        layers.GlobalMaxPooling2D(),\n","        layers.Dense(1),\n","    ],\n","    name=\"discriminator\",\n",")\n","\n","# 생성자를 생성합니다.\n","latent_dim = 128\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape=(latent_dim,)),\n","        # 7x7x128 맵으로 reshape 할 128개의 계수를 생성하려고 합니다.\n","        layers.Dense(7 * 7 * 128),\n","        layers.LeakyReLU(negative_slope=0.2),\n","        layers.Reshape((7, 7, 128)),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(negative_slope=0.2),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(negative_slope=0.2),\n","        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n","    ],\n","    name=\"generator\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"-_7b5EMQzgJa"},"source":["여기 `compile()`을 자체 시그니처로 재정의하고,\n","`train_step`에서 17줄로 전체 GAN 알고리즘을 구현한,\n","기능 완성형(feature-complete) GAN 클래스가 있습니다:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"LQw23NWrzgJa","executionInfo":{"status":"ok","timestamp":1726959781166,"user_tz":-540,"elapsed":2,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}}},"outputs":[],"source":["class GAN(keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super().__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","        self.d_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n","        self.g_loss_tracker = keras.metrics.Mean(name=\"g_loss\")\n","        self.seed_generator = keras.random.SeedGenerator(1337)\n","\n","    @property\n","    def metrics(self):\n","        return [self.d_loss_tracker, self.g_loss_tracker]\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super().compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    def train_step(self, real_images):\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","        # 잠재 공간에서 랜덤 포인트를 샘플링합니다.\n","        batch_size = tf.shape(real_images)[0]\n","        random_latent_vectors = keras.random.normal(\n","            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n","        )\n","\n","        # 그것들을 가짜 이미지로 디코딩합니다.\n","        generated_images = self.generator(random_latent_vectors)\n","\n","        # 그것들을 진짜 이미지와 결합합니다.\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","        # 진짜 이미지와 가짜 이미지를 구분하는 레이블을 구성합니다.\n","        labels = tf.concat(\n","            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n","        )\n","        # 레이블에 랜덤 노이즈를 추가합니다 - 중요한 트릭입니다!\n","        labels += 0.05 * keras.random.uniform(\n","            tf.shape(labels), seed=self.seed_generator\n","        )\n","\n","        # 판별자를 트레이닝합니다.\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(combined_images)\n","            d_loss = self.loss_fn(labels, predictions)\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        self.d_optimizer.apply(grads, self.discriminator.trainable_weights)\n","\n","        # 잠재 공간에서 랜덤 포인트를 샘플링합니다.\n","        random_latent_vectors = keras.random.normal(\n","            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n","        )\n","\n","        # '모두 진짜 이미지(all real images)'라는 레이블을 구성합니다.\n","        misleading_labels = tf.zeros((batch_size, 1))\n","\n","        # 생성자를 트레이닝합니다! (판별자의 가중치는 *업데이트하지 않아야* 한다는 점에 유의하세요)\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply(grads, self.generator.trainable_weights)\n","\n","        # 메트릭을 업데이트하고 그 값을 반환합니다.\n","        self.d_loss_tracker.update_state(d_loss)\n","        self.g_loss_tracker.update_state(g_loss)\n","        return {\n","            \"d_loss\": self.d_loss_tracker.result(),\n","            \"g_loss\": self.g_loss_tracker.result(),\n","        }"]},{"cell_type":"markdown","metadata":{"id":"CyKJAss4zgJb"},"source":["시험해봅시다:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"0AgWkORfzgJb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726959799690,"user_tz":-540,"elapsed":6773,"user":{"displayName":"Tae Kyu Lee","userId":"00886399863979660459"}},"outputId":"385e45c4-5871-4295-9308-36cae75367cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - d_loss: 0.5343 - g_loss: 0.8430\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7f32815145e0>"]},"metadata":{},"execution_count":11}],"source":["# 데이터셋을 준비합니다. 트레이닝 및 테스트 모두 MNIST 숫자 데이터를 사용합니다.\n","batch_size = 64\n","(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n","all_digits = np.concatenate([x_train, x_test])\n","all_digits = all_digits.astype(\"float32\") / 255.0\n","all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n","dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n","gan.compile(\n","    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",")\n","\n","# 실행 시간을 제한하기 위해 100개의 배치에서만 트레이닝합니다.\n","# 전체 데이터셋으로 트레이닝할 수도 있습니다.\n","# 좋은 결과를 얻으려면 약 20 에포크가 필요합니다.\n","gan.fit(dataset.take(100), epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"wHL_dUykzgJb"},"source":["딥러닝의 아이디어는 간단한데, 왜 구현은 어려워야 할까요?"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/guides/ipynb/custom_train_step_in_tensorflow.ipynb","timestamp":1726959461156}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}